\section{Estimadores}
\begin{enumerate}
	\setcounter{enumi}{100}
	\item
		Estimador de momentos, iguala los momentos poblacionales ($E(X^k)$) con los momentos muestrales ($\sum_{i=0}^n\frac{X_i^k}{n}$) y despeja los parámetros buscados.
		
		\begin{itemize}
			\item Exponencial:
			
				$E(X) = 1/\lambda$ y el momento muestral es el promedio, así que $\lambda =1/\overline{X}$
			\item Uniforme$[0,\theta]$:
				$E(X) = \theta/2$ y el muestral es el promedio. $\theta = 2\overline{X}$
		\end{itemize}
	
	\item
		Como la esperanza es $0$ se resuelve usando el segundo momento.
		$$E(X^2) = \int_{-\theta}^{\theta}\frac{1}{2\theta}x^2 = \frac{1}{2\theta} \frac{x^3}{3}\Big|_{-\theta}^{\theta} = \frac{\theta^2}{3}$$
		
	\item
		Sea $X\sim\Gamma(\alpha,\lambda)$.
		Sabemos que $E(X) = \frac{\alpha}{\lambda}$ y $E(X^2) = V(X) + E(X)^2 = \frac{\alpha}{\lambda^2} + \frac{\alpha^2}{\lambda^2}$.
		
		Sea $S = \sum X_i^2 $.
		$$\overline{X} = E(X) = \frac{\alpha}{\lambda} \Rightarrow \frac{1}{\lambda^2} = \frac{\overline{X}^2}{\alpha^2}$$
		
		
		\begin{align*}
			\frac{S}{n}	& = E(X^2) = \frac{\alpha}{\lambda^2} + \frac{\alpha^2}{\lambda^2} = \frac{\alpha \overline{X}^2}{\alpha^2} + \frac{\alpha^2 \overline{X}^2}{\alpha^2}	\\
						& = \frac{\overline{X}^2}{\alpha} + \frac{\alpha \overline{X}^2}{\alpha} = \frac{\overline{X}^2 + \alpha \overline{X}^2}{\alpha}
		\end{align*}
		\begin{align*}
			S								& = \frac{n(\overline{X}^2 + \alpha \overline{X}^2)}{\alpha}	\\
			\alpha S						& = n\overline{X}^2 + n\alpha \overline{X}^2					\\
			\alpha (S - n\overline{X}^2)	& = n\overline{X}^2												\\
			\alpha							& = \frac{n\overline{X}^2}{S - n\overline{X}^2} = \frac{n\overline{X}^2}{\sum X_i^2 - n\overline{X}^2}
		\end{align*}
		
		$$\lambda = \frac{\alpha}{\overline{X}} = \frac{n\overline{X}}{\sum X_i^2 - n\overline{X}^2}$$
		
	\item
		El estimador de máxima verosimilitud es el que maximiza la función de verosimilitud $L$, que es la probabilidad de la muestra obtenida dado el parámetro a estimar.
		Para encontrar el extremo se deriva e iguala a cero. En general se trabaja con $ln(L)$.
		
		\textbf{EMV para la Bernoulli}:
		\begin{align*}
			L(p_0)		& = P(\underline{X} = \underline{x} | p=p_0) = \prod_i P(X_i = x_i|p=p_0) = \prod_i p_0^{x_i}(1-p_0)^{1-x_i}	\\
			l(p_0)		& = \sum_i ln(p_0)\cdot x_i + \sum_i ln(1-p_0)\cdot (1-x_i)		\\
			l(p_0)		& = ln(p_0)\sum_i x_i + ln(1-p_0) \sum_i(1-x_i)
		\end{align*}
		Ahora derivo e igualo a cero:
		\begin{align*}
			0 = l(p_0)'	& = \frac{1}{p_0}\sum_i x_i - \frac{1}{1-p_0} \sum_i(1-x_i)		\\
			\frac{1}{1-p_0} \sum_i(1-x_i)	& = \frac{1}{p_0}\sum_i x_i					\\
			p_0 \sum_i(1-x_i) 				& = (1-p_0)\sum_i x_i						\\
			n\cdot p_0 - \sum_i x_i 		& = \sum_i x_i -p_0\sum_i x_i				\\
			n\cdot p_0 - p_0 \sum_i x_i		& = \sum_i x_i -p_0\sum_i x_i				\\
			n\cdot p_0						& = \sum_i x_i								\\
			p_0								& = \overline{X}
		\end{align*}
	
	\item
		\textbf{EMV para la Exponencial}:
		\begin{align*}
			L(\lambda_0)		& = \prod_i \lambda_0 \cdot e^{-\lambda_0 x_i}	\\
								& = \lambda_0^n \cdot e^{-\lambda_0 \sum_i x_i}	\\
			l(\lambda_0)		& = ln(\lambda_0^n) -\lambda_0 \sum_i x_i	\\
			0 = l(\lambda_0)'	& = \frac{1}{\lambda_0^n}\cdot n\cdot \lambda_0^{n-1} - \sum_i x_i	\\
			0 = l(\lambda_0)'	& = \frac{n}{\lambda_0} - \sum_i x_i
		\end{align*}
		
		Entonces:
			$$\frac{n}{\lambda_0} = \sum_i x_i \Rightarrow \lambda_0 = \frac{1}{\overline{X}}$$
			
	\item
		\textbf{EMV para la Uniforme [$0,\theta$]}:
		$$L(\theta_0) = \prod f_{x_i}(x_i|\theta = \theta_0)
			= \prod_i f_{x_i}(x_i|\theta = \theta_0)
			= \prod_i \frac{1}{\theta_0} \mathbb{I}_{\{x_i < \theta_0\}}
			= \frac{1}{\theta_0^n} \prod_i \mathbb{I}_{\{x_i < \theta_0\}}$$
			
		Como la función es decreciente, y la indicadora da 1 cuando $\theta_0 \geq max\{x_i\}$, la función se maximiza en $\theta_0 =  max\{x_i\}$.
		
	\item
		El sesgo ($b_{\theta}(\overline{\theta})$) es $E(\overline{\theta}) - \theta$.
		Algo es insesgado si su sesgo es 0. Algo es asintóticamente insesgado cuando $\lim_{n\rightarrow\infty}b_{\theta}(\overline{\theta}) = 0$.
		
		\begin{enumerate}
			\item Bernoulli: $E(p_0) = E(\sum_i x_i) / n = \sum_i E(x_i) / n = np/n = p$
			
			(es insesgado)
			\item Promedio como estimador de $\mu_X$:
				$$E(\overline{X}) = \frac{\sum_i E(x_i)}{n} = \frac{\sum_i \mu}{n} = \mu$$ (es insesgado)
			\item Varianza muestral como estimador de la varianza:
				\begin{align*}
					E(\sigma_0^2) 	& = \sum_i \frac{E(X_i - \overline{X})^2}{n} = \sum_i \frac{E(X_i^2 -2\overline{X}X_i + \overline{X}^2)}{n}												\\
									& = \sum_i \frac{E(X_i^2)}{n} - E\left(\sum_i\frac{2\cdot \overline{X}X_i}{n}\right) + \sum_i\frac{E(\overline{X}^2)}{n}								\\
									& = \sum_i \frac{V(X_i) + E(X_i)^2}{n} - E\left(2\cdot \overline{X}\frac{\sum_i X_i}{n}\right) + \sum_i\frac{V(\overline{X}) + E(\overline{X})^2}{n}	\\
									& = \sum_i \frac{\sigma^2 + \mu^2}{n} - E(2\cdot \overline{X}^2) + \sum_i\frac{\sigma^2/n + \mu^2}{n}													\\
									& = \sigma^2 + \mu^2 - 2\cdot E(\cdot \overline{X}^2) + \frac{\sigma^2}{n} + \mu^2																		\\
									& = \frac{n+1}{n}\sigma^2 + 2\mu^2 - 2\cdot \left(\frac{\sigma^2}{n} + \mu^2\right) = \frac{n-1}{n}\sigma^2
				\end{align*}
				(es asintóticamente insesgado)
		\end{enumerate}
		
	\item
		\textbf{Media:}
		\begin{align*}
			L(\mu_0)	& = P(\underline X = \underline x|\mu=\mu_0)	\\
						& = \prod_i \frac{1}{\sqrt{2\pi\sigma^2}} e^{\frac{-(x_i - \mu_0)^2}{2\sigma^2}}		\\
						& = (2\pi\sigma^2)^{-\frac{n}{2}} \cdot e^{\frac{-\sum_i(x_i - \mu_0)^2}{2\sigma^2}}	\\
			l(\mu_0)	& = -\frac{n}{2}ln(2\pi\sigma^2) - \frac{\sum_i(x_i - \mu_0)^2}{2\sigma^2}				\\
			l(\mu_0)'	& = \frac{\sum_i 2(x_i - \mu_0)}{2\sigma^2}	= \frac{\sum_i (x_i - \mu_0)}{\sigma^2}		\\
		\end{align*}
		
		Igualo a cero:
			$$0 = \frac{\sum_i (x_i - \mu_0)}{\sigma^2}
				\Rightarrow 0 = \sum_i (x_i - \mu_0)
				\Rightarrow 0 = \sum_i x_i - n\mu_0
				\Rightarrow \mu_0 = \overline X$$
				
		\textbf{Varianza:}
		\begin{align*}
			L(\sigma_0)		& = P(\underline X = \underline x|\sigma=\sigma_0)	\\
							& = (2\pi\sigma_0^2)^{-\frac{n}{2}} \cdot e^{\frac{-\sum_i(x_i - \mu)^2}{2\sigma_0^2}}	\\
			l(\sigma_0)		& = -\frac{n}{2}ln(2\pi\sigma_0^2) - \frac{\sum_i(x_i - \mu)^2}{2\sigma_0^2}			\\
			l(\sigma_0)'	& = -\frac{n}{2}\frac{1}{2\pi\sigma_0^2}\cdot 2\pi + \frac{\sum_i(x_i - \mu)^2}{2}\cdot \frac{1}{(\sigma_0^2)^2}	\\
			l(\sigma_0)'	& = -\frac{n}{2\sigma_0^2} + \frac{\sum_i(x_i - \mu)^2}{2(\sigma_0^2)^2}
		\end{align*}
		
		Igualo a cero:
		\begin{align*}
			0			& = \frac{\sum_i(x_i - \mu)^2}{2(\sigma_0^2)^2} - \frac{n}{2\sigma_0^2}	\\
			0			& = \frac{\sum_i(x_i - \mu)^2}{\sigma_0^2} - n	\\
			n			& = \frac{\sum_i(x_i - \mu)^2}{\sigma_0^2}		\\
			\sigma_0^2	& = \frac{\sum_i(x_i - \mu)^2}{n}
		\end{align*}
		
	\item
		Está hecho en el 107. Sesgo: $\dfrac{n-1}{n}\sigma^2 - \sigma^2 = -\dfrac{\sigma^2}{n}$.
	\item
		\begin{align*}
			ECM_{\theta}(\hat\theta)	& = E_{\theta} [(\hat\theta - \theta)^2]						\\
										& = E_{\theta} [\hat\theta^2 - 2\hat\theta \theta + \theta^2]	\\
										& = E_{\theta} (\hat\theta^2) - E_{\theta}(2\hat\theta \theta) + E_{\theta}(\theta^2)	\\
										& = E_{\theta} (\hat\theta^2) - 2\theta E_{\theta}(\hat\theta) + \theta^2	\\
										& = E_{\theta} (\hat\theta^2) - E_{\theta}(\hat\theta)^2 + E_{\theta}(\hat\theta)^2 - 2\theta E_{\theta}(\hat\theta) + \theta^2	\\
										& = V_{\theta}(\hat\theta) + (E_{\theta}(\hat\theta)-\theta)^2 = V_{\theta}(\hat\theta) + (b_{\theta}(\hat\theta)^2
		\end{align*}
	
	\item
		$$F_T(t) = P(T<t) = P(X < e^t) = F_X(e^t)$$
		$$f_T(t) = f_X(e^t) \cdot e^t
			= a\cdot (e^t)^{-(a+1)}\cdot e^t \mathbb{I}_{\{e^t \geq 1\}}
			= a\cdot e^{-at} e^{-t} \cdot e^t \mathbb{I}_{\{t \geq 0\}}
			= a\cdot e^{-at} \mathbb{I}_{\{t \geq 0\}}$$
			
		Con lo que $T\sim E(a)$.
		
		\textbf{Estimador de momentos:}
		$$\overline{X} = \frac{1}{\hat a} \Rightarrow \hat a = \frac{1}{\overline X}$$
		
		\textbf{Estimador de máxima verosimilitud:}
		\begin{align*}
			L(\hat a)	& = \prod_i P(X_i = x_i | a = \hat a) = \prod_i \hat a\cdot e^{-\hat a x_i}		\\
						& = \hat a^n \cdot e^{-\hat a \sum_i x_i}										\\
			l(\hat a)	& = n\cdot ln(\hat a) - \hat a \sum_i x_i										\\
			l(\hat a)'	& = \frac{n}{\hat a} - \sum_i x_i
		\end{align*}
		Igualo a cero y queda\footnote{Es todo raro porque son la misma función con dos métodos distintos. Se supone que sea así?}:
		$$\hat a = \frac{1}{\overline X}$$
		
	\item
	\textbf{Sesgo:}
		$$b(\overline X^2) = E(\overline X^2) - \mu^2 = V(\overline X) + E(\overline X)^2 - \mu^2
			= \frac{\sigma^2}{n} + \mu^2 - \mu^2 = \frac{\sigma^2}{n}$$
			
		Es asintóticamente insesgado.
		
		\textbf{Consistencia:}
		$$V\left(\overline X^2\right) = \frac{1}{n^4} \cdot V\left(\sum X_i\right)^2
			= \frac{1}{n^4} \cdot V\left(\sum_i \sum_j X_i X_j\right) = \frac{1}{n^4} \cdot \sum_i \sum_j V(X_i X_j)$$
		
		Si $i=j$:
		$$V(X_i^2) = E(X_i^4) - E(X_i^2)^2$$
		
		Si $i\neq j$, $X_i$ y $X_j$ son independientes:
		\begin{align*}
			V(X_iX_j)	& = E(X_i^2X_j^2) - E(X_iX_j)^2	\\
						& = E(X_i^2)E(X_j^2) + \text{Cov}(X_i^2,X_j^2) - (\text{Cov}(X_i, X_j) + E(X_i)E(X_j))^2	\\
						& = E(X_i^2)E(X_j^2) - (E(X_i)E(X_j))^2
		\end{align*}
		
		En ambos casos quedan sumas de momentos de $X_i$, que como son normales son todos finitos.
		Sea $M$ el máximo resultado de la suma de arriba.
		$$lim_{n\rightarrow \infty}V\left(\overline X^2\right)\leq lim_{n\rightarrow \infty}\frac{n^2\cdot M}{n^4} = lim_{n\rightarrow \infty}\frac{M}{n^2} = 0$$
		Luego el estimador es consistente.
	\item
		\begin{align*}
			L_{\theta}(\hat \theta)		& = \prod_i \left(2\hat \theta x_i e^{-\hat \theta x_i^2}\right)		\\
										& = (2\hat \theta)^n \prod_i (x_i) e^{-\hat \theta \sum_i x_i^2}			\\
			l_{\theta}(\hat \theta)		& = n\cdot ln(2\hat \theta) + \sum_i ln(x_i) - \hat \theta \sum_i x_i^2	\\
			l_{\theta}(\hat \theta)'	& = \frac{n}{\hat \theta} - \sum_i x_i^2
		\end{align*}
		
		Igualo a cero:
		\begin{align*}
			\frac{n}{\hat \theta}	& = \sum_i x_i^2	\\
			\hat \theta				& = \frac{n}{\sum_i x_i^2}
		\end{align*}
	
	\item
		\textbf{Momentos:}
			$$\frac{\theta}{2} = \overline{X} \Rightarrow \theta = 2\overline X$$
			$$b_\theta(2\overline X) = E_\theta(2\overline X) - \theta = 2 \frac{\theta}{2} - \theta = 0$$
			El estimador de momentos es insesgado.
			
		\textbf{EMV:}
			El EMV es $M=max\{X_i\}$
			$$F_M(m) = P(M\leq m) = \prod_i \frac{m}{\theta} = \frac{m^n}{\theta^n}$$
			$$f_M(m) = \frac{n\cdot m^{n-1}}{\theta^n}$$
			
			$$E(M) = \int_0^{\theta} m\cdot \frac{n\cdot m^{n-1}}{\theta^n}
				= \frac{n}{\theta^n} \int_0^{\theta} m^n
				= \frac{n}{\theta^n} \frac{\theta^{n+1}}{n+1}
				= \frac{n}{n+1}\theta$$
			El EMV es asintóticamente insesgado.
\end{enumerate}
